# AutoMLOps Copilot ğŸš€

**DigitalOcean Gradientâ„¢ AI Hackathon Project**

Automatically convert any GitHub ML repository into a production-ready, deployed inference API using DigitalOcean's full AI and cloud ecosystem.

## ğŸ¯ Project Vision

AutoMLOps Copilot allows users to paste a GitHub ML repository URL and automatically:
- Understand the repository structure using AI (Groq/Gemini)
- Generate Docker and API layers
- Train the model on Gradientâ„¢ GPU (TODO)
- Store model artifacts in DigitalOcean Spaces (TODO)
- Deploy a public inference API (TODO)
- Provide monitoring and logs (TODO)

## ğŸ—ï¸ Architecture

```
User â†’ Frontend (Next.js) â†’ Orchestrator (Go) â†’ Worker (Python) â†’ AI Agent
                                â†“                    â†“
                          PostgreSQL              Redis Queue
                                                     â†“
                                              Generate Artifacts:
                                              - Dockerfile
                                              - Training Script
                                              - FastAPI Service
```

## ğŸ“¦ Tech Stack

- **Frontend**: Next.js 14 + TypeScript + Tailwind CSS
- **API Orchestrator**: Go (Gin framework)
- **AI Agent**: Python + Groq LLM (llama-3.3-70b-versatile)
- **Async Workers**: Python + Redis
- **Database**: PostgreSQL
- **Queue**: Redis
- **Containerization**: Docker + Docker Compose
- **Deployment** (TODO): Kubernetes (DOKS), Gradient GPU, DO Spaces

## ğŸš€ Current Status

### âœ… Completed
- [âœ…] Next.js frontend with real-time job updates
- [âœ…] Go API orchestrator with job management
- [âœ…] Python AI agent that analyzes repos
- [âœ…] Groq LLM integration for code generation
- [âœ…] PostgreSQL database for job metadata
- [âœ…] Redis queue for async processing
- [âœ…] Worker that generates Dockerfile, training scripts, FastAPI services
- [âœ…] Docker Compose for local development

### ğŸš§ In Progress / TODO
- [ ] Add "View Generated Code" in UI
- [ ] Kubernetes manifests for build/training/deployment
- [ ] Docker image building pipeline
- [ ] Model training on Gradient GPU
- [ ] DigitalOcean Spaces integration
- [ ] Inference API deployment
- [ ] Monitoring (Prometheus + Grafana)
- [ ] Production deployment to DOKS

## ğŸ“‚ Project Structure

```
automlops-copilot/
â”œâ”€â”€ frontend/              # Next.js UI
â”‚   â”œâ”€â”€ app/              # Next.js 14 app directory
â”‚   â”œâ”€â”€ lib/              # API client
â”‚   â””â”€â”€ package.json
â”‚
â”œâ”€â”€ orchestrator/          # Go API service
â”‚   â”œâ”€â”€ cmd/server/       # Main entry point
â”‚   â”œâ”€â”€ internal/         # Internal packages
â”‚   â”‚   â”œâ”€â”€ handlers/     # HTTP handlers
â”‚   â”‚   â”œâ”€â”€ models/       # Data models
â”‚   â”‚   â”œâ”€â”€ database/     # Database connection
â”‚   â”‚   â”œâ”€â”€ queue/        # Redis queue
â”‚   â”‚   â””â”€â”€ config/       # Configuration
â”‚   â””â”€â”€ go.mod
â”‚
â”œâ”€â”€ agent/                 # Python AI repo parser
â”‚   â”œâ”€â”€ src/
â”‚   â”‚   â”œâ”€â”€ analyzer/     # Repo analysis
â”‚   â”‚   â”œâ”€â”€ llm/          # LLM client (Groq/Gemini)
â”‚   â”‚   â”œâ”€â”€ generators/   # Code generators
â”‚   â”‚   â””â”€â”€ main.py
â”‚   â”œâ”€â”€ requirements.txt
â”‚   â””â”€â”€ .env.example
â”‚
â”œâ”€â”€ workers/              # Async job processing
â”‚   â”œâ”€â”€ src/worker.py
â”‚   â”œâ”€â”€ requirements.txt
â”‚   â””â”€â”€ .env.example
â”‚
â”œâ”€â”€ k8s-manifests/        # Kubernetes YAML files (TODO)
â”‚   â”œâ”€â”€ build/
â”‚   â”œâ”€â”€ training/
â”‚   â””â”€â”€ inference/
â”‚
â”œâ”€â”€ docker-compose.yml    # Local development setup
â””â”€â”€ README.md
```

## ğŸ› ï¸ Setup Instructions

### Prerequisites
- Docker & Docker Compose
- Go 1.21+
- Python 3.10+
- Node.js 18+
- PostgreSQL 15+
- Redis 7+

### 1. Clone Repository
```bash
git clone https://github.com/AyaanShaheer/automlops-copilot.git
cd automlops-copilot
```

### 2. Setup Agent (Python)
```bash
cd agent
python3 -m venv .venv
source .venv/bin/activate  # On Windows: .venv\Scripts\activate
pip install -r requirements.txt

# Create .env file
cp .env.example .env
# Add your GROQ_API_KEY and/or GEMINI_API_KEY
```

### 3. Setup Orchestrator (Go)
```bash
cd orchestrator
cp .env.example .env
go mod download
go build -o bin/orchestrator ./cmd/server
```

### 4. Setup Workers (Python)
```bash
cd workers
python3 -m venv .venv
source .venv/bin/activate
pip install -r requirements.txt

# Copy API keys from agent/.env
cp .env.example .env
# Add your API keys
```

### 5. Setup Frontend (Next.js)
```bash
cd frontend
npm install
cp .env.local.example .env.local
```

### 6. Start Services
```bash
# Terminal 1: Start PostgreSQL and Redis
docker-compose up -d postgres redis

# Terminal 2: Start Orchestrator
cd orchestrator
./bin/orchestrator

# Terminal 3: Start Worker
cd workers
source .venv/bin/activate
python3 src/worker.py

# Terminal 4: Start Frontend
cd frontend
npm run dev
```

### 7. Access Application
- Frontend: http://localhost:3000
- API: http://localhost:8080
- Health Check: http://localhost:8080/health

## ğŸ® Usage

1. Open http://localhost:3000
2. Paste any GitHub ML repository URL
3. Click "Create Job"
4. Watch as the AI agent:
   - Clones and analyzes the repo
   - Generates production-ready Dockerfile
   - Generates training wrapper script
   - Generates FastAPI inference service
5. View generated artifacts in `/tmp/automlops-output/{job_id}/`

## ğŸ“Š Example Repos to Test

- https://github.com/ageron/handson-ml2
- https://github.com/tensorflow/models
- https://github.com/keras-team/keras-examples
- Your own ML repositories!

## ğŸ”‘ Environment Variables

### Agent (.env)
```env
GROQ_API_KEY=your_groq_key
GEMINI_API_KEY=your_gemini_key
LLM_PROVIDER=groq
TEMP_REPO_DIR=/tmp/repos
```

### Orchestrator (.env)
```env
SERVER_PORT=8080
DB_HOST=localhost
DB_PORT=5432
DB_NAME=automlops
REDIS_HOST=localhost
REDIS_PORT=6379
```

## ğŸ› Troubleshooting

### Worker not picking up jobs
```bash
# Check Redis connection
docker exec -it automlops-redis redis-cli ping

# Check queue
docker exec -it automlops-redis redis-cli LLEN automlops:jobs
```

### Database connection issues
```bash
# Check PostgreSQL
docker exec -it automlops-postgres psql -U postgres -d automlops -c "\dt"
```

### LLM API errors
- Verify API keys in `agent/.env` and `workers/.env`
- Check rate limits on Groq/Gemini dashboard

## ğŸ“ Generated Artifacts

For each job, the system generates:
- `Dockerfile` - Production-ready containerization
- `training_wrapper.py` - Training orchestration script
- `app.py` - FastAPI inference service
- `requirements.txt` - Python dependencies
- `analysis.json` - Repository analysis metadata

## ğŸ¯ Roadmap

### Phase 1: Core Foundation âœ…
- [] Basic UI and API
- [] Job queue system
- [] AI code generation

### Phase 2: Execution Pipeline ğŸš§
- [ ] Docker image building
- [ ] Model training
- [ ] Inference deployment

### Phase 3: Production ğŸ”œ
- [ ] Kubernetes deployment
- [ ] GPU training on Gradient
- [ ] DigitalOcean integration
- [ ] Monitoring & logging

## ğŸ‘¥ Team & Responsibilities

- **Member 1**: Frontend UI and dashboard (Saif)
- **Member 2**: Go orchestrator and job system, Kubernetes, Docker, and deployment pipelines (Ayaan)
- **Member 3 & 4**: Python AI agent and repo understanding (Zain and Afzaal)

## ğŸ† Why This Project Wins

1. Demonstrates full ML lifecycle automation
2. Uses complete DigitalOcean ecosystem
3. Production-ready system design
4. Real GPU usage (when implemented)
5. Impressive infrastructure and automation

## ğŸ“„ License

MIT License - See LICENSE file for details

## ğŸ¤ Contributing

This is a hackathon project. After the hackathon, we welcome contributions!

## ğŸ“§ Contact

For questions or issues, please open a GitHub issue.

---

